{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K3lQBqTImOl"
      },
      "source": [
        "В домашней работе вам необходимо создать интерфейс к вашей модели нейронной сети, используя Streamlit (можно и Gradio). Для этого:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yISV-SjhKvGi"
      },
      "source": [
        "1. Определитесь, какую задачу будет решать ваша нейронная сеть.\n",
        "2. Продумайте интерфейс взаимодействия с пользователем, какими параметрами модели пользователь будет управлять.\n",
        "3. Обучите модель на любом публичном датасете или возьмите из любого предыдущего урока. Вспомните как происходит загрузка и выгрузка моделей в Keras.\n",
        "4. Загрузите обученную модель в Colab с интерфейсом (деплой модели).\n",
        "5. Создайте интерфейс для инференса вашей модели (для запросов к модели).\n",
        "6. Изучите как происходит загрузка файлов для моделей с помощью Streamlit по [ссылке](https://docs.streamlit.io/develop/api-reference/widgets/st.file_uploader).\n",
        "7. Добавьте в интерфейс возможность загрузки пользовательских данных для инференса. Это может быть текстовый файл, картинка, аудиофайл или др.\n",
        "8. Выполнив задание, получите 3 балла.\n",
        "9. Вы также можете получить дополнительные 2 балла, если реализуете в одном интерфейсе обучение модели и её инференс."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAgM1XzjSdsM"
      },
      "source": [
        "**Инференс** - это процесс исполнения обученных моделей машинного обучения для получения предсказаний на данных, поданных на вход модели.\n",
        "Обычно нейронная сеть проходит три жизненных этапа: обучение, деплой и инференс. Инференсом называется непрерывная работа какой-либо нейронной сети на конечном устройстве.\n",
        "\n",
        "**Деплой** - загрузка на сервер."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UktayYIyFCx3",
        "outputId": "cb903260-432a-4dc0-ee59-ae05f4d527d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.36.0-py2.py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.0.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.4.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<5,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m942.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.6.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Installing collected packages: watchdog, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 pydeck-0.9.1 smmap-5.0.1 streamlit-1.36.0 watchdog-4.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vL9xmoD63c6o",
        "outputId": "b9b44393-edda-4dcc-a5a4-c7b17f88bee8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "import os\n",
        "import shutil\n",
        "from keras import layers, Input, Model\n",
        "from keras import models\n",
        "import keras\n",
        "import numpy as np\n",
        "from keras import optimizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications import MobileNet\n",
        "from keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "import streamlit as st\n",
        "\n",
        "IMAGE_PATH = './temp/PetImages/'\n",
        "BASE_DIR = './dataset/'\n",
        "\n",
        "# Создание путей для тренировки, валид. и тестов\n",
        "train_dir = os.path.join(BASE_DIR, 'train')\n",
        "validation_dir = os.path.join(BASE_DIR, 'validation')\n",
        "test_dir = os.path.join(BASE_DIR, 'test')\n",
        "\n",
        "if not os.path.exists(BASE_DIR):\n",
        "    os.mkdir(BASE_DIR)\n",
        "    os.mkdir(train_dir)\n",
        "    os.mkdir(validation_dir)\n",
        "    os.mkdir(test_dir)\n",
        "\n",
        "IMG_WIDTH = 224\n",
        "IMG_HEIGHT = 224\n",
        "\n",
        "st.set_page_config(layout=\"wide\")\n",
        "leftl_column, left_column, right_column = st.columns((1, 2, 2))\n",
        "leftl_column.subheader('Информационное окно')\n",
        "left_column.subheader('Обучение модели')\n",
        "right_column.subheader('Работа с обученной моделью')\n",
        "\n",
        "\n",
        "# Скачать и разархивировать датасет\n",
        "def get_dataset():\n",
        "    leftl_column.write('Идет загрузка файла')\n",
        "    response = requests.get(st.session_state.url)\n",
        "    file_Path = 'dataset.zip'\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        with open(file_Path, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        leftl_column.write('Файл скачан успешно')\n",
        "\n",
        "        leftl_column.write('Идет разархивирование файла')\n",
        "        with zipfile.ZipFile(file_Path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(\"temp\")\n",
        "        leftl_column.write('Разархивирование файла закончено')\n",
        "\n",
        "        CLASS_LIST = sorted(os.listdir(IMAGE_PATH))\n",
        "        leftl_column.write(f\"Датасет содержит классы: {CLASS_LIST}\")\n",
        "\n",
        "    else:\n",
        "        leftl_column.write('Ошибка загрузки')\n",
        "\n",
        "\n",
        "def create_dataset(img_path: str, new_path: str, class_name: str, start_index: int, end_index: int):\n",
        "    src_path = os.path.join(img_path, class_name)  # Полный путь к папке с изображениями класса\n",
        "    dst_path = os.path.join(new_path, class_name)  # Полный путь к папке с новым датасетом класса\n",
        "\n",
        "    # Получение списка имен файлов с изображениями текущего класса\n",
        "    class_files = os.listdir(src_path)\n",
        "    # Создаем подпапку, используя путь\n",
        "    if not os.path.exists(dst_path):\n",
        "        os.mkdir(dst_path)\n",
        "\n",
        "    for fname in class_files[start_index: end_index]:\n",
        "        src = os.path.join(src_path, fname)\n",
        "        dst = os.path.join(dst_path, fname)\n",
        "        shutil.copyfile(src, dst)\n",
        "\n",
        "\n",
        "# Ввод URL\n",
        "left_column.text_input(\"Введите URL адрес Датасета (записан по умолчанию)\",\n",
        "                       \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\",\n",
        "                       key=\"url\")\n",
        "\n",
        "# Кнопка загрузки датасет\n",
        "if left_column.button('Загрузить датасет'):\n",
        "    get_dataset()\n",
        "\n",
        "# Кнопка содания выборок\n",
        "left_column.divider()\n",
        "btn_create = left_column.button('Создать выборки')\n",
        "# бегунок выбора процентов для выборки\n",
        "percent = left_column.slider('Выберите процент для тренировочной выборки', value=70)\n",
        "\n",
        "if btn_create:\n",
        "    if os.path.exists(\"temp\"):\n",
        "        leftl_column.write(\"Запуск создания выборок\")\n",
        "\n",
        "        CLASS_LIST = sorted(os.listdir(IMAGE_PATH))\n",
        "        NUM_CLASSES = len(CLASS_LIST)\n",
        "\n",
        "        num_skipped = 0 # счетчик поврежденных файлов\n",
        "        for folder_name in os.listdir(IMAGE_PATH): # перебираем папки\n",
        "            folder_path = os.path.join(IMAGE_PATH, folder_name) # склеиваем путь\n",
        "            for fname in os.listdir(folder_path): # получаем список файлов в папке\n",
        "                fpath = os.path.join(folder_path, fname) # получаем путь до файла\n",
        "                try:\n",
        "                    fobj = open(fpath, \"rb\") # пытаемся открыть файл для бинарного чтения (rb)\n",
        "                    is_jfif = b\"JFIF\" in fobj.peek(10)\n",
        "                finally:\n",
        "                    fobj.close()\n",
        "\n",
        "                if not is_jfif:\n",
        "                    num_skipped += 1\n",
        "                    os.remove(fpath)\n",
        "        leftl_column.write(f\"Удалено изображений без признака b'JFIF': {num_skipped}\")\n",
        "\n",
        "        # Расчет процентов (на валидацию и тестовую, пусть распределяется поровну)\n",
        "        dataset_len = sum(len(subdir[2]) for subdir in os.walk(IMAGE_PATH))\n",
        "        train_len = int(dataset_len * percent / 100)\n",
        "        val_test_len = int((dataset_len - train_len) * 50 / 100)\n",
        "\n",
        "        for class_label in range(NUM_CLASSES):\n",
        "            class_name = CLASS_LIST[class_label]\n",
        "            class_path = IMAGE_PATH + class_name\n",
        "            class_len = os.listdir(class_path)\n",
        "            leftl_column.write(f'Размер класса {class_name} составляет {len(class_len)} животных')\n",
        "\n",
        "            # Создание выборок\n",
        "            create_dataset(IMAGE_PATH, train_dir, class_name, 0, train_len // NUM_CLASSES)\n",
        "            create_dataset(IMAGE_PATH, validation_dir, class_name, train_len // NUM_CLASSES, (train_len + val_test_len) // NUM_CLASSES)\n",
        "            create_dataset(IMAGE_PATH, test_dir, class_name, (train_len + val_test_len) // NUM_CLASSES, None)\n",
        "\n",
        "        leftl_column.write(f'Общий размер базы для обучения: {dataset_len}')\n",
        "\n",
        "        leftl_column.write(f\"Число кошек {len(os.listdir(os.path.join(train_dir, 'Cat')))}, \"\n",
        "         f\"число собак {len(os.listdir(os.path.join(train_dir, 'Dog')))} в обучающей выборке\")\n",
        "\n",
        "        leftl_column.write(f\"Число кошек {len(os.listdir(os.path.join(validation_dir, 'Cat')))}, \"\n",
        "         f\"число собак {len(os.listdir(os.path.join(validation_dir, 'Dog')))} в проверочной выборке\")\n",
        "\n",
        "        leftl_column.write(f\"Число кошек {len(os.listdir(os.path.join(test_dir, 'Cat')))}, \"\n",
        "         f\"число собак {len(os.listdir(os.path.join(test_dir, 'Dog')))} в контрольной выборке\")\n",
        "\n",
        "        if not os.path.exists(\"model_maker\"):\n",
        "          os.mkdir(\"model_maker\")\n",
        "\n",
        "        leftl_column.write(\"Выборки созданы\")\n",
        "\n",
        "    else:\n",
        "        leftl_column.write(\"Сначала загрузите Датасет\")\n",
        "\n",
        "\n",
        "def model_maker():\n",
        "    CLASS_LIST = sorted(os.listdir(IMAGE_PATH))\n",
        "    NUM_CLASSES = len(CLASS_LIST)\n",
        "\n",
        "    base_model = MobileNet(include_top=False, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "\n",
        "    for layer in base_model.layers[:]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    input = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "    custom_model = base_model(input)\n",
        "    custom_model = GlobalAveragePooling2D()(custom_model)\n",
        "    custom_model = Dense(64, activation='relu')(custom_model)\n",
        "    custom_model = Dropout(0.5)(custom_model)\n",
        "    predictions = Dense(NUM_CLASSES, activation='softmax')(custom_model)\n",
        "\n",
        "    return Model(inputs=input, outputs=predictions)\n",
        "\n",
        "# Кнопка содания модели и запуска обучения + выбор параметров\n",
        "left_column.divider()\n",
        "btn_start = left_column.button('Запустить обучение')\n",
        "epochs = left_column.slider('Выберите количество эпох', 1, 100, value = 1)\n",
        "batch_size = left_column.slider('Выберите размер батча', 16, 256, step = 16, value = 256)\n",
        "learning_rate = left_column.radio('Скорость обучения:', [1e-03, 1e-04, 1e-05])\n",
        "\n",
        "if btn_start:\n",
        "    if os.path.exists(\"model_maker\"):\n",
        "        leftl_column.write(\"Создание модели\")\n",
        "        model = model_maker()\n",
        "        model.summary()\n",
        "\n",
        "        train_datagen = ImageDataGenerator(\n",
        "        rescale=1. / 255,\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "        )\n",
        "\n",
        "        test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "        train_generator = train_datagen.flow_from_directory(train_dir, target_size=(IMG_WIDTH, IMG_HEIGHT), batch_size=batch_size,\n",
        "                                                            class_mode='categorical')\n",
        "        validation_generator = test_datagen.flow_from_directory(validation_dir, target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "                                                                batch_size=batch_size, class_mode='categorical')\n",
        "\n",
        "        leftl_column.write(\"Компиляция модели\")\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=learning_rate), metrics=['acc'])\n",
        "\n",
        "        leftl_column.write(\"Обучение началось\")\n",
        "        history = model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n",
        "        leftl_column.write(\"Обучение закончилось\")\n",
        "\n",
        "\n",
        "        def show_history(store):\n",
        "            acc = store.history['acc']\n",
        "            val_acc = store.history['val_acc']\n",
        "            loss = store.history['loss']\n",
        "            val_loss = store.history['val_loss']\n",
        "            epochs = range(1, len(acc) + 1)\n",
        "\n",
        "            fig, axs = plt.subplots(2, 1, figsize=(9, 15), sharey=True)\n",
        "\n",
        "            axs[0].set_title(\"График точности на проверочной и обучающей выборках\")\n",
        "            axs[0].plot(epochs, acc, 'r', label='Точность на обучающей выборке')\n",
        "            axs[0].plot(epochs, val_acc, 'bo', label='Точность на проверочной выборке')\n",
        "\n",
        "            axs[1].set_title(\"График потерь на проверочной и обучающей выборках\")\n",
        "            axs[1].plot(epochs, loss, 'r', label='Потери на обучающей выборке')\n",
        "            axs[1].plot(epochs, val_loss, 'bo', label='Потери на проверочной выборке')\n",
        "            leftl_column.pyplot(fig)\n",
        "\n",
        "\n",
        "        show_history(history)\n",
        "\n",
        "        leftl_column.write(\"Запуск контрольной выборки\")\n",
        "        test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical'\n",
        "        )\n",
        "\n",
        "        test_loss, test_acc = model.evaluate(test_generator)\n",
        "        leftl_column.write(f'Точность на контрольной выборке: {test_acc}')\n",
        "\n",
        "        model_name = 'my_model.h5'\n",
        "        model.save(model_name)\n",
        "        leftl_column.write(f\"Файл {model_name} сохранен в Colab\")\n",
        "\n",
        "    else:\n",
        "        leftl_column.write(\"Сначала создайте выборки\")\n",
        "\n",
        "# Кнопка сохранить модель на компьютер\n",
        "# if left_column.button('Сохранить модель'):\n",
        "    # if os.path.exists(\"my_model.h5\"):\n",
        "          # files.download('my_model.h5')\n",
        "    # else:\n",
        "    #     leftl_column.write(\"Сначала пройдите обучение\")\n",
        "\n",
        "# Кнопка загрузить модель с компьютера\n",
        "data_file = right_column.file_uploader(\"Загрузить файл c названием 'my_model.h5' (у вас так же должен быть подгружен датасет, для понимания количества классов)\",\n",
        "                                       type=[\"h5\"])\n",
        "\n",
        "if data_file:\n",
        "    if os.path.exists(\"temp\"):\n",
        "        file_details = {\"filename\": data_file.name, \"filetype\": data_file.type, \"filesize\": data_file.size}\n",
        "        leftl_column.write(file_details)\n",
        "\n",
        "        with open(data_file.name, \"wb\") as f:\n",
        "            f.write(data_file.getbuffer())\n",
        "\n",
        "        leftl_column.write(\"Модель сохранена в Colab, можно тестировать модель\")\n",
        "\n",
        "    else:\n",
        "        leftl_column.write(\"Сначала загрузите датасет!\")\n",
        "\n",
        "# Запуск теста модели\n",
        "right_column.divider()\n",
        "image_file = right_column.file_uploader(\"Загрузить файл c кошкой или собакой\", type=[\"png\",\"jpg\",\"jpeg\"])\n",
        "\n",
        "if image_file:\n",
        "    if os.path.exists(\"my_model.h5\"):\n",
        "        CLASS_LIST = sorted(os.listdir(IMAGE_PATH))\n",
        "\n",
        "        with open(image_file.name, \"wb\") as f:\n",
        "            f.write(image_file.getbuffer())\n",
        "\n",
        "        img = keras.utils.load_img(image_file.name, target_size=(IMG_WIDTH, IMG_HEIGHT)) # Загружаем картинку\n",
        "        leftl_column.image(img)\n",
        "\n",
        "        img_array = keras.utils.img_to_array(img) # Преобразуем картинку в тензор\n",
        "        img_array = keras.backend.expand_dims(img_array, 0)  # Создание дополнительного измерения для батча\n",
        "\n",
        "        # Загрузить сохраненную модель\n",
        "        model = model_maker()\n",
        "        model = keras.saving.load_model('my_model.h5')\n",
        "        # model.summary()\n",
        "\n",
        "        predictions = model.predict(img_array)\n",
        "        leftl_column.write(predictions)\n",
        "\n",
        "        leftl_column.write(\"Предсказание: %s \\n Вероятность: %2.1f%%\" %\n",
        "        (CLASS_LIST[np.argmax(predictions)], np.max(predictions)*100))\n",
        "\n",
        "    else:\n",
        "        leftl_column.write(\"Сначала запустите обучение или загрузите готовую модель!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "phWjuvbG3zNA",
        "outputId": "67ab4a32-2dd8-4842-eeb4-e29c53635233"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ssh: connect to host serveo.net port 22: Connection refused\r\n"
          ]
        }
      ],
      "source": [
        "!streamlit run main.py --server.address=localhost >/content/logs.txt & ssh -o \"StrictHostKeyChecking no\" -R 80:localhost:8501 serveo.net"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
